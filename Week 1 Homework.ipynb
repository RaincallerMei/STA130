{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.1 points]: All relevant ChatBot summaries [including link(s) to chat log histories if you're using ChatGPT] are reported within the notebook\n",
    "# [0.2 points]: Reasonable well-written general definitions for Question \"2.2\"\n",
    "# [0.3 points]: Demonstrated understanding regarding Question \"4\"\n",
    "# [0.4 points]: Requested assessment of ChatBot versus google performance in Question \"8.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b622ab86",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "1. Pick one of the datasets from the ChatBot session(s) of the TUT demo (or from your own ChatBot session if you wish) and use the code produced through the ChatBot interactions to import the data and confirm that the dataset has missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1813ab8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived         0\n",
       "pclass           0\n",
       "sex              0\n",
       "age            177\n",
       "sibsp            0\n",
       "parch            0\n",
       "fare             0\n",
       "embarked         2\n",
       "class            0\n",
       "who              0\n",
       "adult_male       0\n",
       "deck           688\n",
       "embark_town      2\n",
       "alive            0\n",
       "alone            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the dataset\n",
    "url = ' https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "\n",
    "# Read the CSV data from the URL\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "# Calculate the sum of missing variables\n",
    "titanic_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff3ff2c",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Start a new ChatBot session with an initial prompt introducing the dataset you're using and request help to determine how many columns and rows of data a pandas DataFrame has, and then \n",
    "1. use code provided in your ChatBot session to print out the number of rows and columns of the dataset; and,\n",
    "2. write your own general definitions of the meaning of \"observations\" and \"variables\" based on asking the ChatBot to explain these terms in the context of your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58b13b9a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "rows, columns = titanic_df.shape\n",
    "print(rows)\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6604a94",
   "metadata": {},
   "source": [
    "## Observations\n",
    "An observation refers to a single record or entry in a dataset that has been measured. It represents a complete data point for one instance of the study or analysis, usually contained in a row of a dataset. In the case of the Titannic data set, each observation corresponds to an individual passenger, with information about that individual such as their name, age, ticket class, and whether they survived.\n",
    "\n",
    "## Variables\n",
    "A variable represents a specific characteristic being measured or recorded for each entity. These are usually contained in the columns of a dataset. In the case of my choosen data set, the variables include characteristics of each passenger, such as: Survived, Ticket class, Age, etc;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6850bf",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Ask the ChatBot how you can provide simple summaries of the columns in the dataset and use the suggested code to provide these summaries for your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4b4fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         survived      pclass         age       sibsp       parch        fare\n",
      "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
      "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
      "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
      "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n"
     ]
    }
   ],
   "source": [
    "# Get the summary statistics of numerical columns\n",
    "# Ex: count, mean, standard deviation, min, max, and percentiles.\n",
    "numerical_summary = titanic_df.describe()\n",
    "\n",
    "print(numerical_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d09270f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "24.00    30\n",
      "22.00    27\n",
      "18.00    26\n",
      "19.00    25\n",
      "28.00    25\n",
      "         ..\n",
      "36.50     1\n",
      "55.50     1\n",
      "0.92      1\n",
      "23.50     1\n",
      "74.00     1\n",
      "Name: count, Length: 88, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the frequency of unique values in the 'Pclass' column (ticket class)\n",
    "age_counts = titanic_df['age'].value_counts()\n",
    "\n",
    "print(age_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd815b1",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "If the dataset you're using has (a) non-numeric variables and (b) missing values in numeric variables, explain (perhaps using help from a ChatBot if needed) the discrepancies between size of the dataset given by df.shape and what is reported by df.describe() with respect to (a) the number of columns it analyzes and (b) the values it reports in the \"count\" column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ccd54f",
   "metadata": {},
   "source": [
    "The Titanic dataset contains both numerical and categorical columns.\n",
    "\n",
    "While df.shape includes all types of columns (numerical, categorical, and others), df.describe() by default only provides summary statistics for numerical columns (like int, float). It excludes categorical columns, such as those containing strings or non-numeric data.\n",
    "\n",
    "This leads to fewer columns being summarized in .describe() compared to .shape. The result from Question 2 shows that .shape claims there are 15 columns in the dataframe, but only 6 of them are numerical as Question 3 shows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd134f0d",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12afbbf5",
   "metadata": {},
   "source": [
    "An \"attribute\", such as df.shape, is a static property that stores some value and can be directly accessed and needs no further computations.\n",
    "\n",
    "A \"method\", such as df.describe() which does end with (), is a function that may require parameters to be passed into the \"()\" for further computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5591608",
   "metadata": {},
   "source": [
    "## Summary 1\n",
    "In our conversation, I helped you with various aspects of working with pandas DataFrames, particularly using the Titanic dataset. We discussed how to determine the number of rows and columns using df.shape, and the meaning of \"observations\" (rows) and \"variables\" (columns) in the dataset. I demonstrated how to summarize data using df.describe() for numerical columns and df['column'].value_counts() for categorical columns. We also explored the discrepancies between df.shape and df.describe(), especially regarding missing values and the number of columns analyzed. Finally, I explained the difference between attributes like df.shape (which do not require parentheses) and methods like df.describe() (which do).\n",
    "\n",
    "https://chatgpt.com/share/66e3aa68-39b0-8002-af3a-6cbbea18c6fa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf107d",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "The df.describe() method provides the 'count', 'mean', 'std', 'min', '25%', '50%', '75%', and 'max' summary statistics for each variable it analyzes. Give the definitions (perhaps using help from the ChatBot if needed) of each of these summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1bd2df",
   "metadata": {},
   "source": [
    "Count:\n",
    "\n",
    "    The number of non-null (non-missing) entries for each column or variable. It indicates how many valid data points are available for each variable.\n",
    "Mean:\n",
    "\n",
    "    The arithmetic average of the values in a column. It is calculated by summing all the values and dividing by the count (total number of non-null values). It provides a central value for the data.\n",
    "Std (Standard Deviation):\n",
    "\n",
    "    This measures the amount of variation or dispersion in the data. A higher standard deviation means the values are spread out over a wider range, while a lower value means they are clustered closely around the mean.\n",
    "Min (Minimum):\n",
    "\n",
    "    The smallest value in the dataset for the column. It provides the lower boundary of the data range.\n",
    "25% (1st Quartile or Q1):\n",
    "\n",
    "    This is the value below which 25% of the data falls. It is the median of the lower half of the dataset and marks the boundary of the first quartile.\n",
    "50% (Median or 2nd Quartile or Q2):\n",
    "\n",
    "    The middle value of the dataset. Half of the data lies below this value and half lies above. It is more robust to outliers than the mean.\n",
    "75% (3rd Quartile or Q3):\n",
    "\n",
    "    This is the value below which 75% of the data falls. It marks the upper boundary of the third quartile and represents the median of the upper half of the dataset.\n",
    "Max (Maximum):\n",
    "\n",
    "    The largest value in the dataset for the column. It provides the upper boundary of the data range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b03fd17",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "Missing data can be considered \"across rows\" or \"down columns\". Consider how df.dropna() or del df['col'] should be applied to most efficiently use the available non-missing data in your dataset and briefly answer the following questions in your own words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22b0e1",
   "metadata": {},
   "source": [
    "1. Provide an example of a \"use case\" in which using df.dropna() might be peferred over using del df['col']\n",
    "\n",
    "By using .dropna(), only the empty values are dropped from the set, while using del df['col'], everything, including potentially useful datas with the deleted variables (column), are dropped. \n",
    "\n",
    "For example, suppose you have a dataset containing customer information, including columns for age, income, and purchase history. If you notice that most of the columns have some amount of missing data, you might prefer using df.dropna(), so that you keep the variables of the remaining good entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d8f596",
   "metadata": {},
   "source": [
    "2. Provide an example of \"the opposite use case\" in which using del df['col'] might be preferred over using df.dropna()\n",
    "\n",
    "If one of the columns have too many missing values, deleting an entire column will eliminate a column that may not contribute valuable information to the data analysis.\n",
    "\n",
    "For example, suppose you have another dataset containing customer information, but this time, only the columns for age is having a lot of missing data, enough that we can't pull useful conclusions out from the limited data. This time, it is better to use del df['col'] to remove the 'age' column entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da0e334",
   "metadata": {},
   "source": [
    "3. Discuss why applying del df['col'] before df.dropna() when both are used together could be important\n",
    "\n",
    "First, removing unnecessary columns reduces the number of columns that df.dropna() needs to evaluate, which can speed up the computation.\n",
    "\n",
    "Second, if you apply df.dropna() first, you might end up dropping much more rows that could be important just because of the missing values in columns you intended to remove later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa1ff32",
   "metadata": {},
   "source": [
    "4. Remove all missing data from one of the datasets you're considering using some combination of del df['col'] and/or df.dropna() and give a justification for your approach, including a \"before and after\" report of the results of your approach for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eddfa02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#before cleaning\n",
    "print(titanic_df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a2dab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Columns with Excessive Missing Data:\n",
    "# print(titanic_df.columns)\n",
    "\n",
    "del titanic_df['deck']\n",
    "del titanic_df['age']\n",
    "\n",
    "#Remove Rows with Missing Values:\n",
    "\n",
    "titanic_df_cleaned = titanic_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5533f58",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n",
      "891\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "#after cleaning\n",
    "\n",
    "print(titanic_df_cleaned.isna().sum())\n",
    "\n",
    "# rows, columns = titanic_df.shape\n",
    "# print(rows)\n",
    "# print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5fa9e6",
   "metadata": {},
   "source": [
    "## Summary 2\n",
    "In our discussion, we explored methods for handling missing data in a dataset using pandas. We examined `df.dropna()` and `del df['col']` as strategies for managing incomplete data. `df.dropna()` is preferred when you want to keep all columns but discard rows with missing values, while `del df['col']` is useful for removing columns with excessive missing data. Applying `del df['col']` before `df.dropna()` can enhance efficiency and prevent unintended data loss. We also reviewed a practical example where removing a column with significant missing data and then dropping rows with remaining missing values led to a cleaner dataset, allowing for a more accurate analysis.\n",
    "\n",
    "https://chatgpt.com/share/66e3b61d-2e7c-8002-9d15-9091222a5ab7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e250a0c",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "Give brief explanations in your own words for any requested answers to the questions below:\n",
    "\n",
    "1. Use your ChatBot session to understand what df.groupby(\"col1\")[\"col2\"].describe() does and then demonstrate and explain this using a different example from the \"titanic\" data set other than what the ChatBot automatically provide for you.\n",
    "\n",
    "The df.groupby(\"col1\")[\"col2\"].describe() method in pandas is used to generate descriptive statistics for a specific column (col2) within each group defined by another column (col1).\n",
    "\n",
    "Group By: The df.groupby(\"col1\") part groups the DataFrame by the unique values in col1.\n",
    "\n",
    "Select Column: The [\"col2\"] part selects the column for which you want to compute the statistics.\n",
    "\n",
    "Describe: The describe() method then computes various descriptive statistics for col2 within each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group by the \"class\" column and describe the \"age\" column\n",
    "result = df.groupby(\"class\")[\"age\"].describe()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d436e",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "Loading the Data: We load the Titanic dataset from the URL and create a DataFrame (df).\n",
    "\n",
    "Grouping: We group the DataFrame by the \"class\" column, which contains the class of each passenger (e.g., First, Second, Third).\n",
    "\n",
    "Descriptive Statistics: For each class, we compute statistics on the \"age\" column. This includes metrics like count, mean, standard deviation, minimum, and maximum age."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9849275",
   "metadata": {},
   "source": [
    "2. Assuming you've not yet removed missing values in the manner of question \"7\" above, df.describe() would have different values in the count value for different data columns depending on the missingness present in the original data. Why do these capture something fundamentally different from the values in the count that result from doing something like df.groupby(\"col1\")[\"col2\"].describe()?\n",
    "\n",
    "First, df.describe() gives a global view of each column's statistics across the entire DataFrame., while df.groupby(\"col1\")[\"col2\"].describe() provides a breakdown of statistics for col2 within each group defined by col1.\n",
    "\n",
    "Second, in df.describe(), missing values in a column reduce the overall count for that column.\n",
    "In df.groupby(\"col1\")[\"col2\"].describe(), missing values are counted separately for each group. Different groups might have different numbers of non-missing values due to variations in the presence of missing data within those groups.\n",
    "\n",
    "Third, df.describe() aggregates statistics over the whole dataset. df.groupby(\"col1\")[\"col2\"].describe() aggregates statistics separately for each group, so the count can vary significantly if col1 has a non-uniform distribution or many missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba3f42",
   "metadata": {},
   "source": [
    "3. Intentionally introduce the following errors into your code and report your opinion as to whether it's easier to (a) work in a ChatBot session to fix the errors, or (b) use google to search for and fix errors: first share the errors you get in the ChatBot session and see if you can work with ChatBot to troubleshoot and fix the coding errors, and then see if you think a google search for the error provides the necessary toubleshooting help more quickly than ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4ee552",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#     Forget to include import pandas as pd in your code\u001b[39;00m\n\u001b[1;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m titanic_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      6\u001b[0m titanic_df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ChatGPT: \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#     Forget to include import pandas as pd in your code\n",
    "\n",
    "url = ' https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "\n",
    "titanic_df = pd.read_csv(url)\n",
    "titanic_df.isna().sum()\n",
    "    \n",
    "# ChatGPT: The error \"name 'pd' is not defined\" occurs because the pandas library (pd) has not been imported in your script. \n",
    "# You need to import pandas at the beginning of your code before you can use it.\n",
    "\n",
    "# Online: https://stackoverflow.com/questions/40407090/nameerror-name-pd-is-not-defined\n",
    "\n",
    "#Both worked well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b0b7e6c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m titanic_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m titanic_df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "#  Mistype \"titanic.csv\" as \"titanics.csv\"\n",
    "import pandas as pd\n",
    "url = ' https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv'\n",
    "\n",
    "titanic_df = pd.read_csv(url)\n",
    "titanic_df.isna().sum()\n",
    "\n",
    "\"\"\"\n",
    "ChatGPT: Even helps to find the correct link\n",
    "The HTTP Error 404 indicates that the URL you are trying to access is not found. This error typically occurs if the URL is incorrect or the resource has been moved or deleted.\n",
    "\n",
    "In your case, the URL might be incorrect. The dataset you are trying to access is from the Seaborn library, and the correct URL for the Titanic dataset is:\n",
    "\n",
    "Online: No direct solution to this specific problem. First poped up link: https://github.com/jupyter/help/issues/196, no help\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ccdc937",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Try to use a dataframe before it's been assigned into the variable\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m titanic_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43murl\u001b[49m)\n\u001b[1;32m      6\u001b[0m titanic_df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      7\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": [
    "#Try to use a dataframe before it's been assigned into the variable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "titanic_df = pd.read_csv(url)\n",
    "titanic_df.isna().sum()\n",
    "url = ' https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "\n",
    "\"\"\"\n",
    "ChatGPT: \"Hereâ€™s how to correct your code: Define and assign the url variable before you use it in pd.read_csv(url). Ensure the URL does not have any extra spaces.\"\n",
    "Online: https://stackoverflow.com/questions/40484963/django-url-mapping-nameerror-name-x-is-not-defined, somewhat helpful response:\n",
    "You have the NameError because you are referencing myapp in myproject/urls.py but haven't imported it.\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8e1f9e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (2600589447.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    titanic_df = pd.read_csv(url\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "#Forget one of the parentheses somewhere the code\n",
    "\n",
    "import pandas as pd\n",
    "url = ' https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "titanic_df = pd.read_csv(url\n",
    "titanic_df.isna().sum()\n",
    "\n",
    "\"\"\"\n",
    "ChatGPT: The SyntaxError: '(' was never closed error occurs because you have an unmatched opening parenthesis in your code. Specifically, in your pd.read_csv(url line, the opening parenthesis ( is not closed.\n",
    "\n",
    "Online: the Jupyter Notebook alreay pointed out\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e415e628",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'group_by'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_270/1485958613.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtitanic_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtitanic_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtitanic_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtitanic_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescrible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \"\"\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'group_by'"
     ]
    }
   ],
   "source": [
    "#Mistype one of the names of the chained functions with the code\n",
    "\n",
    "import pandas as pd\n",
    "url = ' https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "titanic_df = pd.read_csv(url)\n",
    "titanic_df.isna().sum()\n",
    "titanic_df.group_by(\"name\")[\"age\"].describe()\n",
    "titanic_df.groupby(\"name\")[\"age\"].describle()\n",
    "\n",
    "\"\"\"\n",
    "ChatGPT:\n",
    "The error AttributeError: 'DataFrame' object has no attribute 'group_by' occurs because the method group_by does not exist. The correct method for grouping data in a pandas DataFrame is groupby.\n",
    "\n",
    "Additionally, there's a typo in the second line of your code where you used describle instead of describe.\n",
    "\n",
    "Here's how you should correct your code:\n",
    "\n",
    "Replace group_by with groupby.\n",
    "Correct the typo describle to describe.\n",
    "\n",
    "Online: No direct related help\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eca6c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBoth are helpful.\\n\\nChatGPT: troubleshoot: \\nVerify Column Names: After stripping whitespace, verify the column names again:\\n\\nCheck for Whitespace: Strip any extra whitespace from column names:\\n\\nPrint Column Names: Print the column names of your DataFrame to check for any discrepancies:\\n\\nOnline: https://rollbar.com/blog/python-keyerror/ \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use a column name that's not in your data for the groupby and column selection\n",
    "import pandas as pd\n",
    "url = ' https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "titanic_df = pd.read_csv(url)\n",
    "titanic_df.isna().sum()\n",
    "titanic_df.groupby(\"Sex\")[\"age\"].describe()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Both are helpful.\n",
    "\n",
    "ChatGPT: troubleshoot: \n",
    "Verify Column Names: After stripping whitespace, verify the column names again:\n",
    "\n",
    "Check for Whitespace: Strip any extra whitespace from column names:\n",
    "\n",
    "Print Column Names: Print the column names of your DataFrame to check for any discrepancies:\n",
    "\n",
    "Online: https://rollbar.com/blog/python-keyerror/ \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65646525",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m titanic_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      6\u001b[0m titanic_df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m----> 7\u001b[0m titanic_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[43msex\u001b[49m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m      8\u001b[0m titanic_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m)[age]\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mGPT: The NameError: name 'sex' is not defined occurs because you have used sex without quotes, which means Python is trying to interpret it as a variable rather than a column name.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mOnline: too many NameError cases, no direct help.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sex' is not defined"
     ]
    }
   ],
   "source": [
    "#Forget to put the column name as a string in quotes for the groupby and column selection, and see if the ChatBot and google are still as helpful as they were for the previous question\n",
    "\n",
    "import pandas as pd\n",
    "url = ' https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "titanic_df = pd.read_csv(url)\n",
    "titanic_df.isna().sum()\n",
    "titanic_df.groupby(sex)[\"age\"].describe()\n",
    "titanic_df.groupby(\"sex\")[age].describe()\n",
    "\n",
    "\"\"\"\n",
    "GPT: The NameError: name 'sex' is not defined occurs because you have used sex without quotes, which means Python is trying to interpret it as a variable rather than a column name.\n",
    "\n",
    "Similarly, in titanic_df.groupby(\"sex\")[age].describe(), you need to use the column name 'age' in quotes.\n",
    "\n",
    "Hereâ€™s how you should fix your code:\n",
    "\n",
    "Use Column Names in Quotes: Always use column names as strings in quotes when referring to them in DataFrame operations.\n",
    "\n",
    "Online: too many NameError cases, no direct help.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afda56d",
   "metadata": {},
   "source": [
    "#### Summary \n",
    "\n",
    "To fix the errors in your code, start by ensuring you correctly import the necessary libraries and verify URLs for accuracy. Use exact column names in quotes for DataFrame operations, and handle missing values or typos appropriately. For instance, use pd.read_csv(url) correctly with a defined url variable, and make sure to use groupby(\"column_name\") with the column names in quotes. Address issues like KeyError by checking column names with print(titanic_df.columns), and resolve SyntaxError by closing all parentheses. Finally, make sure the column names and URLs are accurate and free from extra spaces.\n",
    "\n",
    "https://chatgpt.com/share/66e3c3ee-9604-8002-9159-0eed70d02e07\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ff49b",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "9. Have you reviewed the course wiki-textbook and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?\n",
    "\n",
    "Yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
